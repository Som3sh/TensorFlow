{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 2 quickstart for beginners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short introduction uses Keras to:\n",
    "\n",
    "- Load a prebuilt dataset.\n",
    "\n",
    "- Build a neural network machine learning model that classifies images.\n",
    "\n",
    "- Train this neural network.\n",
    "\n",
    "- Evaluate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.16.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "print(\"TensorFlow version: \",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train , y_train) ,(x_test , y_test) = mnist.load_data()\n",
    "\n",
    "x_train , x_test = x_train / 255.0 , x_test / 255.0 # we divide by 255 as the pixel values range from 0 to 255 so we convert it to 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/someone./Library/Python/3.9/lib/python/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# building a tf.keras.Sequential Model :\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (28,28)),\n",
    "    tf.keras.layers.Dense(128,activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation for the above model**\n",
    "\n",
    "This code snippet defines a neural network model using TensorFlow's Keras API. Here's a step-by-step explanation:\n",
    "\n",
    "1. **Sequential Model Creation**: [`tf.keras.Sequential`](command:_github.copilot.openSymbolFromReferences?%5B%7B%22%24mid%22%3A1%2C%22path%22%3A%22%2FUsers%2Fsomeone.%2FLibrary%2FPython%2F3.9%2Flib%2Fpython%2Fsite-packages%2Ftensorflow%2F__init__.py%22%2C%22scheme%22%3A%22file%22%7D%2C%7B%22line%22%3A0%2C%22character%22%3A0%7D%5D \"../../../../../Library/Python/3.9/lib/python/site-packages/tensorflow/__init__.py\") is used to instantiate a linear stack of layers. This means that you're creating a model where the output of one layer is the input to the next layer.\n",
    "\n",
    "2. **Flatten Layer**: The first layer in the model is a `Flatten` layer with an `input_shape` of `(28,28)`. This layer transforms the input 2D image data (28x28 pixels) into a 1D array of 784 pixels (28*28). This is necessary because the next layer, a dense layer, requires a 1D input.\n",
    "\n",
    "3. **Dense Layer with ReLU Activation**: The next layer is a `Dense` layer with 128 neurons (or units). The activation function used is 'ReLU' (Rectified Linear Unit). This layer takes the flattened input and applies a linear transformation followed by the ReLU activation function. The ReLU function is commonly used to introduce non-linearity to the model, allowing it to learn more complex patterns.\n",
    "\n",
    "4. **Dropout Layer**: After the dense layer, a `Dropout` layer is added with a rate of 0.2. This means 20% of the neurons' outputs are randomly set to zero during training. Dropout is a regularization technique used to prevent overfitting by reducing the model's dependency on any single neuron.\n",
    "\n",
    "5. **Output Dense Layer**: The final layer is another `Dense` layer with 10 neurons. Since there's no activation function specified, it defaults to a linear activation. This layer is typically used for the output layer in a classification problem where the model predicts among 10 different classes.\n",
    "\n",
    "In summary, this model takes 28x28 pixel images as input, flattens the images, processes them through a dense layer with ReLU activation, applies dropout for regularization, and finally outputs a prediction through a dense layer with 10 units corresponding to the model's predictions for 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33061513, -0.02871897, -0.20637833,  0.18547878, -0.02577453,\n",
       "        -0.41960594, -0.2418351 , -0.3305199 , -0.24498361,  0.99577665]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(x_train[:1]).numpy() # use numpy to convert the tensor array to numpy array\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12646079, 0.08828747, 0.0739167 , 0.10937648, 0.08854781,\n",
       "        0.05972264, 0.07134178, 0.06528728, 0.0711175 , 0.24594152]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tf.nn.softmax function converts these logits to probabilities for each class:\n",
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "  It is possible to bake the tf.nn.softmax function into the activation function for the last layer of the network. While this can make the model output more directly interpretable, this approach is discouraged as it's impossible to provide an exact and numerically stable loss calculation for all models when using a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function for training using losses.SparseCategoricalCrossentropy:\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8180442"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(y_train[:1] , predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = \"adam\" , \n",
    "    loss = loss_fn , \n",
    "    metrics = [ 'accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 689us/step - accuracy: 0.8600 - loss: 0.4765\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 650us/step - accuracy: 0.9553 - loss: 0.1523\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 651us/step - accuracy: 0.9675 - loss: 0.1105\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636us/step - accuracy: 0.9721 - loss: 0.0887\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641us/step - accuracy: 0.9767 - loss: 0.0736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31992ac10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train , y_train , epochs =  5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - 426us/step - accuracy: 0.9783 - loss: 0.0729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07294681668281555, 0.9782999753952026]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test , y_test , verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Softmax()\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[1.2760117e-07, 1.3531744e-07, 6.7518653e-05, 6.9327140e-04,\n",
       "        3.5755121e-10, 5.6356782e-07, 4.7921373e-13, 9.9920386e-01,\n",
       "        1.9552006e-06, 3.2539785e-05]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_model(x_test[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above probability model we get to know how confident the model is.\n",
    "Basically each of the probabilities is for the accurate representation of the handwritten digit which is from 0-9 for the specific test iteration for each iteration the pictures change \n",
    "\n",
    "So for example in this iteration that is 1 , the probability that the model thinks this is digit 0 is 0.000000127 \n",
    "at the same time the probability for the digit 7 is 0.992 so it is confident that it is digit 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[2.9436975e-09, 4.2220386e-06, 9.9994385e-01, 4.6836933e-05,\n",
       "        3.1305858e-17, 4.9276300e-06, 1.9876470e-08, 6.8474184e-14,\n",
       "        9.2532467e-08, 1.2304565e-11]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_model(x_test[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
